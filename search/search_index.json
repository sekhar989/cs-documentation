{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Layout","text":"<p>Keeping it empty for the time-being to make sure the layout is defined properly.</p>"},{"location":"#contents","title":"Contents","text":""},{"location":"#h1","title":"H1","text":"<p>Keeping it empty for the time-being to make sure the layout is defined properly.</p>"},{"location":"#h3","title":"H3","text":"<p>Keeping it empty for the time-being to make sure the layout is defined properly.</p>"},{"location":"#h3_1","title":"H3","text":"<p>Keeping it empty for the time-being to make sure the layout is defined properly.</p> <p>Lorem Ipsum Keeping it empty for the time-being to make sure the layout is defined properly.</p>"},{"location":"Data%20Science/a.%20Statistics/","title":"Statistics Primer","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nnp.random.seed(518123)\nnumbers = np.random.random(size=(10))\n</pre> import numpy as np np.random.seed(518123) numbers = np.random.random(size=(10)) In\u00a0[2]: Copied! <pre>numbers\n</pre> numbers Out[2]: <pre>array([0.68325401, 0.22474258, 0.35022261, 0.92769926, 0.1806196 ,\n       0.84943356, 0.01967161, 0.04814318, 0.11858102, 0.97782543])</pre> In\u00a0[3]: Copied! <pre>def mean(arr):\n    \"\"\"\n    mean = sum(x_i)/n\n    \"\"\"\n    n = len(arr)\n    sumArr = 0\n    for i in arr:\n        sumArr += i\n    return sumArr/n\n\nmean(numbers), numbers.mean()\n</pre> def mean(arr):     \"\"\"     mean = sum(x_i)/n     \"\"\"     n = len(arr)     sumArr = 0     for i in arr:         sumArr += i     return sumArr/n  mean(numbers), numbers.mean() Out[3]: <pre>(0.43801928681498853, 0.43801928681498853)</pre> In\u00a0[4]: Copied! <pre>def median(arr):\n    \"\"\"\n    n = len(arr)\n    i = (n-1)// 2\n    n is even\n        median = (arr[i] + arr[i + 1]) / 2\n    n is odd\n        median = arr[i]\n    \"\"\"\n    n = len(arr)\n    arr = sorted(arr)\n    i = (n-1)// 2\n    if n % 2 == 0:\n        return (arr[i] + arr[i+1])/2\n    return arr[i]\n\nmedian(numbers), np.median(numbers), median(numbers[1:]), np.median(numbers[1:])\n</pre> def median(arr):     \"\"\"     n = len(arr)     i = (n-1)// 2     n is even         median = (arr[i] + arr[i + 1]) / 2     n is odd         median = arr[i]     \"\"\"     n = len(arr)     arr = sorted(arr)     i = (n-1)// 2     if n % 2 == 0:         return (arr[i] + arr[i+1])/2     return arr[i]  median(numbers), np.median(numbers), median(numbers[1:]), np.median(numbers[1:]) Out[4]: <pre>(0.2874825959506015,\n 0.2874825959506015,\n 0.22474258141813663,\n 0.22474258141813663)</pre> In\u00a0[5]: Copied! <pre>from collections import Counter\n</pre> from collections import Counter In\u00a0[6]: Copied! <pre>numbers = np.random.randint(low=1, high=10, size=(10))\nnumbers\n</pre> numbers = np.random.randint(low=1, high=10, size=(10)) numbers Out[6]: <pre>array([3, 6, 2, 3, 7, 2, 2, 9, 3, 9])</pre> In\u00a0[7]: Copied! <pre>Counter(numbers), dict(sorted(Counter(numbers).items(), key=lambda x: x[1]))\n</pre> Counter(numbers), dict(sorted(Counter(numbers).items(), key=lambda x: x[1])) Out[7]: <pre>(Counter({3: 3, 2: 3, 9: 2, 6: 1, 7: 1}), {6: 1, 7: 1, 9: 2, 3: 3, 2: 3})</pre> In\u00a0[8]: Copied! <pre>counts = Counter(numbers)\n</pre> counts = Counter(numbers) In\u00a0[9]: Copied! <pre>counts.most_common(1)\n</pre> counts.most_common(1) Out[9]: <pre>[(3, 3)]</pre> In\u00a0[10]: Copied! <pre>import pandas as pd\n\n\ndata = [7, 7, 31, 31,47, 75, 87, 115, 116, 119, 119, 155, 177]\ndf = pd.DataFrame(data, index=range(1, len(data)+1), columns=[\"x[i]\"])\ndf\n</pre> import pandas as pd   data = [7, 7, 31, 31,47, 75, 87, 115, 116, 119, 119, 155, 177] df = pd.DataFrame(data, index=range(1, len(data)+1), columns=[\"x[i]\"]) df Out[10]: x[i] 1 7 2 7 3 31 4 31 5 47 6 75 7 87 8 115 9 116 10 119 11 119 12 155 13 177 In\u00a0[11]: Copied! <pre>q2 = median(data)\nq2\n</pre> q2 = median(data) q2 Out[11]: <pre>87</pre> In\u00a0[12]: Copied! <pre>n = len(data)\nmedian_idx = (n - 1)//2\nprint(median_idx, data[median_idx])\n</pre> n = len(data) median_idx = (n - 1)//2 print(median_idx, data[median_idx]) <pre>6 87\n</pre> In\u00a0[13]: Copied! <pre>q1 = int(median(data[:median_idx])); q3 = int(median(data[median_idx + 1:]))\n</pre> q1 = int(median(data[:median_idx])); q3 = int(median(data[median_idx + 1:])) In\u00a0[14]: Copied! <pre>print(f\"Q1: {q1}, Q2: {q2}, Q3: {q3}\")\n</pre> print(f\"Q1: {q1}, Q2: {q2}, Q3: {q3}\") <pre>Q1: 31, Q2: 87, Q3: 119\n</pre> <p>For the data in this table the interquartile range is $IQR = Q3 \u2212 Q1 = 119 - 31 = 88.$</p> In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n</pre> import matplotlib.pyplot as plt plt.style.use(\"ggplot\") In\u00a0[16]: Copied! <pre>plt.boxplot(data, vert=False);\nplt.vlines(x=q1, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q1\", color=\"r\");\nplt.vlines(x=q2, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q2\", color=\"g\");\nplt.vlines(x=q3, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q3\", color=\"b\");\nplt.tight_layout()\nplt.legend();\n</pre> plt.boxplot(data, vert=False); plt.vlines(x=q1, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q1\", color=\"r\"); plt.vlines(x=q2, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q2\", color=\"g\"); plt.vlines(x=q3, ymax=2, ymin=0, linestyles=\"--\", alpha=0.5, label=\"Q3\", color=\"b\"); plt.tight_layout() plt.legend(); In\u00a0[17]: Copied! <pre>hist_data = np.random.randint(low=1, high=30, size=(100))\nplt.hist(hist_data, edgecolor=\"white\", bins=30, color=\"green\", alpha=0.5);\n</pre> hist_data = np.random.randint(low=1, high=30, size=(100)) plt.hist(hist_data, edgecolor=\"white\", bins=30, color=\"green\", alpha=0.5); In\u00a0[18]: Copied! <pre>normal_dist = np.random.normal(size=1000)\nplt.hist(normal_dist, edgecolor=\"white\", color=\"blue\", alpha=0.5, bins=50);\n</pre> normal_dist = np.random.normal(size=1000) plt.hist(normal_dist, edgecolor=\"white\", color=\"blue\", alpha=0.5, bins=50); In\u00a0[19]: Copied! <pre>pd.Series(normal_dist).plot.kde();\n</pre> pd.Series(normal_dist).plot.kde(); <p> </p> <ul> <li><p>Null hypothesis (H0): The assumption we make initially about the dataset and the statistical test assumes, are called the null hypothesis.</p> </li> <li><p>Alternative hypothesis (H1): The alternative assumption holds the assumption we make after the experimentation fails.</p> </li> <li><p>Type 1 Error: The error happens when we fail to accept the null hypothesis meaning we reject the null hypothesis when it should be accepted. It is also called a false positive. The probability of committing a Type 1 error is called the significance level, and it is denoted by $\\alpha$</p> </li> <li><p>Type 2 Error: The error that happens when we fail to reject the null hypothesis, meaning we accept the null hypothesis when it should be rejected. It is also called a false negative. The probability of committing a Type 2 error is denoted by $\\beta$</p> </li> </ul> <p></p> <p> </p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate independent variable\nX = np.linspace(0, 10, 100)\n# Generate homoscedastic data\ntrue_slope = 2\ntrue_intercept = 1\nerror_std = 2\ny = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X))\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set a random seed for reproducibility np.random.seed(42)  # Generate independent variable X = np.linspace(0, 10, 100) # Generate homoscedastic data true_slope = 2 true_intercept = 1 error_std = 2 y = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X)) In\u00a0[4]: Copied! <pre>import statsmodels.api as sm\n</pre> import statsmodels.api as sm In\u00a0[5]: Copied! <pre># Add a constant to get an intercept\nX_train_sm = sm.add_constant(X)\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y, X_train_sm).fit()\n</pre> # Add a constant to get an intercept X_train_sm = sm.add_constant(X) # Fit the resgression line using 'OLS' lr = sm.OLS(y, X_train_sm).fit() In\u00a0[6]: Copied! <pre>lr.params\n</pre> lr.params Out[6]: <pre>array([0.65437429, 2.02758653])</pre> In\u00a0[7]: Copied! <pre>lr.summary()\n</pre> lr.summary() Out[7]: OLS Regression Results Dep. Variable: y   R-squared:             0.915 Model: OLS   Adj. R-squared:        0.914 Method: Least Squares   F-statistic:           1051. Date: Tue, 23 May 2023   Prob (F-statistic): 3.48e-54 Time: 12:57:22   Log-Likelihood:      -200.97 No. Observations:    100   AIC:                   405.9 Df Residuals:     98   BIC:                   411.2 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const     0.6544     0.362     1.807  0.074    -0.064     1.373 x1     2.0276     0.063    32.416  0.000     1.903     2.152 Omnibus:  0.521   Durbin-Watson:         2.042 Prob(Omnibus):  0.771   Jarque-Bera (JB):      0.518 Skew: -0.167   Prob(JB):              0.772 Kurtosis:  2.885   Cond. No.               11.7 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[21]: Copied! <pre>np.corrcoef(X_train_sm[:, 1], y)\n</pre> np.corrcoef(X_train_sm[:, 1], y) Out[21]: <pre>array([[1.        , 0.95639642],\n       [0.95639642, 1.        ]])</pre> In\u00a0[28]: Copied! <pre>len(y), 0.95639642*0.95639642, 1 - (1 - 0.9146941121888165)*(99)/(99-1)\n</pre> len(y), 0.95639642*0.95639642, 1 - (1 - 0.9146941121888165)*(99)/(99-1) Out[28]: <pre>(100, 0.9146941121888165, 0.9138236439458453)</pre> In\u00a0[22]: Copied! <pre>(2.0276/0.063)\n</pre> (2.0276/0.063) Out[22]: <pre>32.18412698412698</pre> In\u00a0[29]: Copied! <pre># Generate independent variable\nX = np.linspace(0, 10, 100)\n\n# Generate heteroscedastic data\nhetero_std = X + np.random.uniform(0, 0.5, size=len(X))\nhetero_y = 2 * X + np.random.normal(0.0, hetero_std, size=len(X))\n</pre> # Generate independent variable X = np.linspace(0, 10, 100)  # Generate heteroscedastic data hetero_std = X + np.random.uniform(0, 0.5, size=len(X)) hetero_y = 2 * X + np.random.normal(0.0, hetero_std, size=len(X)) In\u00a0[32]: Copied! <pre># Add a constant to get an intercept\nX_train_sm = sm.add_constant(X)\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(hetero_y, X_train_sm).fit()\n</pre> # Add a constant to get an intercept X_train_sm = sm.add_constant(X) # Fit the resgression line using 'OLS' lr = sm.OLS(hetero_y, X_train_sm).fit() In\u00a0[33]: Copied! <pre>lr.summary()\n</pre> lr.summary() Out[33]: OLS Regression Results Dep. Variable: y   R-squared:             0.376 Model: OLS   Adj. R-squared:        0.369 Method: Least Squares   F-statistic:           58.94 Date: Thu, 25 May 2023   Prob (F-statistic): 1.23e-11 Time: 12:15:32   Log-Likelihood:      -332.88 No. Observations:    100   AIC:                   669.8 Df Residuals:     98   BIC:                   675.0 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const     0.8759     1.354     0.647  0.519    -1.811     3.563 x1     1.7958     0.234     7.677  0.000     1.332     2.260 Omnibus: 10.799   Durbin-Watson:         2.296 Prob(Omnibus):  0.005   Jarque-Bera (JB):     25.447 Skew: -0.224   Prob(JB):           2.98e-06 Kurtosis:  5.430   Cond. No.               11.7 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[37]: Copied! <pre>multi_X = np.array([X, hetero_std])\n</pre> multi_X = np.array([X, hetero_std]) In\u00a0[41]: Copied! <pre># Add a constant to get an intercept\nX_train_sm = sm.add_constant(multi_X.reshape(100, 2))\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(hetero_y, X_train_sm).fit()\n</pre> # Add a constant to get an intercept X_train_sm = sm.add_constant(multi_X.reshape(100, 2)) # Fit the resgression line using 'OLS' lr = sm.OLS(hetero_y, X_train_sm).fit() In\u00a0[42]: Copied! <pre>lr.summary()\n</pre> lr.summary() Out[42]: OLS Regression Results Dep. Variable: y   R-squared:             0.133 Model: OLS   Adj. R-squared:        0.115 Method: Least Squares   F-statistic:           7.464 Date: Thu, 25 May 2023   Prob (F-statistic): 0.000966 Time: 12:30:39   Log-Likelihood:      -349.27 No. Observations:    100   AIC:                   704.5 Df Residuals:     97   BIC:                   712.3 Df Model:      2 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const     4.1380     1.687     2.453  0.016     0.790     7.486 x1    -4.8625     5.370    -0.905  0.367   -15.521     5.796 x2     5.8878     5.365     1.097  0.275    -4.760    16.536 Omnibus:  7.674   Durbin-Watson:         1.589 Prob(Omnibus):  0.022   Jarque-Bera (JB):      7.499 Skew:  0.534   Prob(JB):             0.0235 Kurtosis:  3.812   Cond. No.               78.9 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[20]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(518123)\n\n# Generate independent variable\nX = np.linspace(0, 10, 100)\n\n# Generate heteroscedastic data\nhetero_std = X + np.random.uniform(0, 0.5, size=len(X))\nhetero_y = 2 * X + np.random.normal(0.0, hetero_std, size=len(X))\n\n# Generate homoscedastic data\nhomoscedastic_std = np.ones_like(X) * 0.5\nhomoscedastic_y = 2 * X + np.random.normal(0, homoscedastic_std, size=len(X))\n\n# Plot heteroscedastic data\nplt.scatter(X, hetero_y, label='Heteroscedastic Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Heteroscedastic Data')\nplt.legend()\nplt.show()\n\n# Plot homoscedastic data\nplt.scatter(X, homoscedastic_y, label='Homoscedastic Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Homoscedastic Data')\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set a random seed for reproducibility np.random.seed(518123)  # Generate independent variable X = np.linspace(0, 10, 100)  # Generate heteroscedastic data hetero_std = X + np.random.uniform(0, 0.5, size=len(X)) hetero_y = 2 * X + np.random.normal(0.0, hetero_std, size=len(X))  # Generate homoscedastic data homoscedastic_std = np.ones_like(X) * 0.5 homoscedastic_y = 2 * X + np.random.normal(0, homoscedastic_std, size=len(X))  # Plot heteroscedastic data plt.scatter(X, hetero_y, label='Heteroscedastic Data') plt.xlabel('X') plt.ylabel('Y') plt.title('Heteroscedastic Data') plt.legend() plt.show()  # Plot homoscedastic data plt.scatter(X, homoscedastic_y, label='Homoscedastic Data') plt.xlabel('X') plt.ylabel('Y') plt.title('Homoscedastic Data') plt.legend() plt.show() In\u00a0[21]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate independent variable\nX = np.linspace(0, 10, 100)\n\n# Generate homoscedastic data\ntrue_slope = 2\ntrue_intercept = 1\nerror_std = 2\nhomoscedastic_y = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X))\n\n# Calculate residuals\nresiduals = homoscedastic_y - (true_intercept + true_slope * X)\n\n# Plot residuals against predicted values\nplt.scatter(homoscedastic_y, residuals)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Homoscedasticity: Residuals vs. Predicted Values')\nplt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set a random seed for reproducibility np.random.seed(42)  # Generate independent variable X = np.linspace(0, 10, 100)  # Generate homoscedastic data true_slope = 2 true_intercept = 1 error_std = 2 homoscedastic_y = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X))  # Calculate residuals residuals = homoscedastic_y - (true_intercept + true_slope * X)  # Plot residuals against predicted values plt.scatter(homoscedastic_y, residuals) plt.xlabel('Predicted Values') plt.ylabel('Residuals') plt.title('Homoscedasticity: Residuals vs. Predicted Values') plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 plt.show()  In\u00a0[22]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate independent variable\nX = np.linspace(0, 10, 100)\n\n# Generate heteroscedastic data\ntrue_slope = 2\ntrue_intercept = 1\nerror_std = 0.05 + 0.5 * X  # Varying standard deviation based on X\nheteroscedastic_y = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X))\n\n# Calculate residuals\nresiduals = heteroscedastic_y - (true_intercept + true_slope * X)\n\n# Plot residuals against predicted values\nplt.scatter(heteroscedastic_y, residuals)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Heteroscedasticity: Residuals vs. Predicted Values')\nplt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set a random seed for reproducibility np.random.seed(42)  # Generate independent variable X = np.linspace(0, 10, 100)  # Generate heteroscedastic data true_slope = 2 true_intercept = 1 error_std = 0.05 + 0.5 * X  # Varying standard deviation based on X heteroscedastic_y = true_intercept + true_slope * X + np.random.normal(0, error_std, size=len(X))  # Calculate residuals residuals = heteroscedastic_y - (true_intercept + true_slope * X)  # Plot residuals against predicted values plt.scatter(heteroscedastic_y, residuals) plt.xlabel('Predicted Values') plt.ylabel('Residuals') plt.title('Heteroscedasticity: Residuals vs. Predicted Values') plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 plt.show()  <p></p> In\u00a0[23]: Copied! <pre>def pca_scratch(mat, n_components=None):\n    # 1. Mean Calculation\n    mean = np.mean(mat, axis=0)\n    std = np.mean(mat, axis=0)\n    # 2. Mean adjusted\n    mat = (mat - mean)/std\n    # 3. Covariance calculation\n    mat_cov = np.cov(mat.T)\n    # 4. Eigen decomposition\n    eigenVals, eigenVecs = np.linalg.eig(mat_cov)\n    \n    # 5. Sort the eigen-values according to their values in descending order\n    idx = np.argsort(eigenVals)[::-1]\n    eigenVals = eigenVals[idx]\n    \n    # 6. Transpose back the eigen-vectors\n    eigenVecs = eigenVecs.T\n    \n    # 7. Sort the eigen-vectors\n    eigenVecs = eigenVecs[idx]\n    # 8. Filter the n-components if it's passed\n    if n_components is not None:\n        eigenVecs = eigenVecs[:n_components]\n    \n    # 9. Transform the matrix by multiplying back \n    #    the eigen-vectors with the mean adjusted matrix\n    transformed = np.dot(mat, eigenVecs.T)\n    return transformed, eigenVals, eigenVecs, mat_cov\n</pre> def pca_scratch(mat, n_components=None):     # 1. Mean Calculation     mean = np.mean(mat, axis=0)     std = np.mean(mat, axis=0)     # 2. Mean adjusted     mat = (mat - mean)/std     # 3. Covariance calculation     mat_cov = np.cov(mat.T)     # 4. Eigen decomposition     eigenVals, eigenVecs = np.linalg.eig(mat_cov)          # 5. Sort the eigen-values according to their values in descending order     idx = np.argsort(eigenVals)[::-1]     eigenVals = eigenVals[idx]          # 6. Transpose back the eigen-vectors     eigenVecs = eigenVecs.T          # 7. Sort the eigen-vectors     eigenVecs = eigenVecs[idx]     # 8. Filter the n-components if it's passed     if n_components is not None:         eigenVecs = eigenVecs[:n_components]          # 9. Transform the matrix by multiplying back      #    the eigen-vectors with the mean adjusted matrix     transformed = np.dot(mat, eigenVecs.T)     return transformed, eigenVals, eigenVecs, mat_cov In\u00a0[24]: Copied! <pre>mat = np.random.randint(low=1, high=10, size=(10, 5))\nprint(mat)\n</pre> mat = np.random.randint(low=1, high=10, size=(10, 5)) print(mat) <pre>[[9 5 1 3 8]\n [6 8 9 4 1]\n [1 4 7 2 3]\n [1 5 1 8 1]\n [1 2 2 6 7]\n [5 1 1 3 2]\n [5 6 7 4 7]\n [8 1 6 8 5]\n [4 2 6 6 1]\n [9 6 3 4 4]]\n</pre> In\u00a0[25]: Copied! <pre>### Mean Centering\nnp.mean(mat, axis = 0) # Column wise mean\n</pre> ### Mean Centering np.mean(mat, axis = 0) # Column wise mean Out[25]: <pre>array([4.9, 4. , 4.3, 4.8, 3.9])</pre> In\u00a0[26]: Copied! <pre>r, c = mat.shape\nfor c in range(c):\n    print(mat[:, c].mean())\n</pre> r, c = mat.shape for c in range(c):     print(mat[:, c].mean()) <pre>4.9\n4.0\n4.3\n4.8\n3.9\n</pre> In\u00a0[27]: Copied! <pre>mean_adjusted_mat = mat - np.mean(mat, axis = 0)\nmean_adjusted_mat\n</pre> mean_adjusted_mat = mat - np.mean(mat, axis = 0) mean_adjusted_mat Out[27]: <pre>array([[ 4.1,  1. , -3.3, -1.8,  4.1],\n       [ 1.1,  4. ,  4.7, -0.8, -2.9],\n       [-3.9,  0. ,  2.7, -2.8, -0.9],\n       [-3.9,  1. , -3.3,  3.2, -2.9],\n       [-3.9, -2. , -2.3,  1.2,  3.1],\n       [ 0.1, -3. , -3.3, -1.8, -1.9],\n       [ 0.1,  2. ,  2.7, -0.8,  3.1],\n       [ 3.1, -3. ,  1.7,  3.2,  1.1],\n       [-0.9, -2. ,  1.7,  1.2, -2.9],\n       [ 4.1,  2. , -1.3, -0.8,  0.1]])</pre> In\u00a0[28]: Copied! <pre># covariance, function needs samples as columns\ncov_mat = np.cov(mean_adjusted_mat.T)\ncov_mat\n</pre> # covariance, function needs samples as columns cov_mat = np.cov(mean_adjusted_mat.T) cov_mat Out[28]: <pre>array([[10.1       ,  1.44444444,  0.14444444, -1.02222222,  2.54444444],\n       [ 1.44444444,  5.77777778,  2.33333333, -1.55555556, -0.22222222],\n       [ 0.14444444,  2.33333333,  9.12222222, -0.71111111, -1.74444444],\n       [-1.02222222, -1.55555556, -0.71111111,  4.4       , -0.8       ],\n       [ 2.54444444, -0.22222222, -1.74444444, -0.8       ,  7.43333333]])</pre> <pre><code>np.cov()\n\n\nDocstring:\nEstimate a covariance matrix, given data and weights.\n\nCovariance indicates the level to which two variables vary together.\nIf we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,\nthen the covariance matrix element :math:`C_{ij}` is the covariance of\n:math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance\nof :math:`x_i`.\n\nSee the notes for an outline of the algorithm.\n\nParameters\n----------\nm : array_like\n    A 1-D or 2-D array containing multiple variables and observations.\n    Each row of `m` represents a variable, and each column a single\n    observation of all those variables.\n...\n</code></pre> In\u00a0[29]: Copied! <pre># eigenvalues, eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(cov_mat)\nprint(eigenvalues, \"\", sep=\"\\n\")\nprint(eigenvectors)\n</pre> # eigenvalues, eigenvectors eigenvalues, eigenvectors = np.linalg.eig(cov_mat) print(eigenvalues, \"\", sep=\"\\n\") print(eigenvectors) <pre>[12.17216789 11.18847366  3.18051177  5.5037741   4.78840591]\n\n[[-0.82883422 -0.07265647  0.07832895 -0.54857477  0.02617576]\n [-0.20731021 -0.4329396  -0.5979355   0.25712105 -0.58817318]\n [ 0.03065676 -0.82265396  0.08550256  0.10118807  0.55204249]\n [ 0.19709144  0.15803613 -0.77051378 -0.4087483   0.41882322]\n [-0.47986606  0.32488364 -0.1879775   0.67501133  0.41617706]]\n</pre> In\u00a0[21]: Copied! <pre>lr.summary2()\n</pre> lr.summary2() Out[21]: Model: OLS Adj. R-squared: 0.914 Dependent Variable: y AIC: 405.9487 Date: 2023-05-23 12:17 BIC: 411.1591 No. Observations: 100 Log-Likelihood: -200.97 Df Model: 1 F-statistic: 1051. Df Residuals: 98 Prob (F-statistic): 3.48e-54 R-squared: 0.915 Scale: 3.3261 Coef. Std.Err. t P&gt;|t| [0.025 0.975] const 0.6544 0.3620 1.8075 0.0738 -0.0641 1.3728 x1 2.0276 0.0625 32.4162 0.0000 1.9035 2.1517 Omnibus: 0.521 Durbin-Watson: 2.042 Prob(Omnibus): 0.771 Jarque-Bera (JB): 0.518 Skew: -0.167 Prob(JB): 0.772 Kurtosis: 2.885 Condition No.: 12  Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  <ul> <li>eigenvector <code>v = [:,i]</code> represents a column vector</li> <li>we should transpose for easier calculations</li> </ul> In\u00a0[30]: Copied! <pre>eigenvectors = eigenvectors.T\n</pre> eigenvectors = eigenvectors.T In\u00a0[31]: Copied! <pre>print(eigenvalues)\nprint(np.argsort(eigenvalues))\nprint(np.argsort(eigenvalues)[::-1])\n</pre> print(eigenvalues) print(np.argsort(eigenvalues)) print(np.argsort(eigenvalues)[::-1]) <pre>[12.17216789 11.18847366  3.18051177  5.5037741   4.78840591]\n[2 4 3 1 0]\n[0 1 3 4 2]\n</pre> In\u00a0[32]: Copied! <pre>idx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx]\nprint(eigenvalues)\n</pre> idx = np.argsort(eigenvalues)[::-1] eigenvalues = eigenvalues[idx] print(eigenvalues) <pre>[12.17216789 11.18847366  5.5037741   4.78840591  3.18051177]\n</pre> In\u00a0[33]: Copied! <pre>eigenvectors = eigenvectors[idx]\nprint(eigenvectors)\n</pre> eigenvectors = eigenvectors[idx] print(eigenvectors) <pre>[[-0.82883422 -0.20731021  0.03065676  0.19709144 -0.47986606]\n [-0.07265647 -0.4329396  -0.82265396  0.15803613  0.32488364]\n [-0.54857477  0.25712105  0.10118807 -0.4087483   0.67501133]\n [ 0.02617576 -0.58817318  0.55204249  0.41882322  0.41617706]\n [ 0.07832895 -0.5979355   0.08550256 -0.77051378 -0.1879775 ]]\n</pre> In\u00a0[34]: Copied! <pre>mean_adjusted_mat.shape, eigenvectors.shape\n</pre> mean_adjusted_mat.shape, eigenvectors.shape Out[34]: <pre>((10, 5), (5, 5))</pre> In\u00a0[35]: Copied! <pre>transformed_mat = np.dot(mean_adjusted_mat, eigenvectors.T)\ntransformed_mat\n</pre> transformed_mat = np.dot(mean_adjusted_mat, eigenvectors.T) transformed_mat Out[35]: <pre>array([[-6.02891327,  3.03148483,  1.17733728, -1.35014863,  0.0572718 ],\n       [-0.36293328, -6.7467456 , -0.72989835, -1.27127172, -0.74217236],\n       [ 3.19525013, -2.6727019 ,  2.94963441, -0.15883511,  2.25199233],\n       [ 4.94628015,  2.12873178, -1.2028854 , -2.37867802, -3.10608617],\n       [ 2.32548828,  4.23812621,  2.9945041 ,  1.59729992, -0.81361458],\n       [ 0.99486081,  3.10456726, -1.70691584, -1.59922132,  3.26356302],\n       [-2.05998852, -2.21360015,  3.15212617,  1.27187625, -0.92350045],\n       [-1.79249902,  0.53815962, -2.8654073 ,  4.58216568, -0.49043875],\n       [ 2.84080901, -1.21976092, -2.29653591,  1.38693479,  0.89124752],\n       [-4.05835427, -0.18826113, -1.47195916, -2.08012185, -0.38826235]])</pre> In\u00a0[36]: Copied! <pre>n_components = 2\neigen_vecs = eigenvectors[:n_components]\nprint(eigen_vecs)\n</pre> n_components = 2 eigen_vecs = eigenvectors[:n_components] print(eigen_vecs) <pre>[[-0.82883422 -0.20731021  0.03065676  0.19709144 -0.47986606]\n [-0.07265647 -0.4329396  -0.82265396  0.15803613  0.32488364]]\n</pre> In\u00a0[37]: Copied! <pre>print(mean_adjusted_mat.shape, eigen_vecs.shape)\n</pre> print(mean_adjusted_mat.shape, eigen_vecs.shape) <pre>(10, 5) (2, 5)\n</pre> In\u00a0[38]: Copied! <pre>p_components = np.dot(mean_adjusted_mat, eigen_vecs.T)\np_components\n</pre> p_components = np.dot(mean_adjusted_mat, eigen_vecs.T) p_components Out[38]: <pre>array([[-6.02891327,  3.03148483],\n       [-0.36293328, -6.7467456 ],\n       [ 3.19525013, -2.6727019 ],\n       [ 4.94628015,  2.12873178],\n       [ 2.32548828,  4.23812621],\n       [ 0.99486081,  3.10456726],\n       [-2.05998852, -2.21360015],\n       [-1.79249902,  0.53815962],\n       [ 2.84080901, -1.21976092],\n       [-4.05835427, -0.18826113]])</pre> In\u00a0[39]: Copied! <pre>from sklearn.decomposition import PCA\n</pre> from sklearn.decomposition import PCA In\u00a0[40]: Copied! <pre>pca = PCA(svd_solver=\"randomized\")\npca.fit_transform(mat)\n</pre> pca = PCA(svd_solver=\"randomized\") pca.fit_transform(mat) Out[40]: <pre>array([[ 6.02891327, -3.03148483,  1.17733728, -1.35014863,  0.0572718 ],\n       [ 0.36293328,  6.7467456 , -0.72989835, -1.27127172, -0.74217236],\n       [-3.19525013,  2.6727019 ,  2.94963441, -0.15883511,  2.25199233],\n       [-4.94628015, -2.12873178, -1.2028854 , -2.37867802, -3.10608617],\n       [-2.32548828, -4.23812621,  2.9945041 ,  1.59729992, -0.81361458],\n       [-0.99486081, -3.10456726, -1.70691584, -1.59922132,  3.26356302],\n       [ 2.05998852,  2.21360015,  3.15212617,  1.27187625, -0.92350045],\n       [ 1.79249902, -0.53815962, -2.8654073 ,  4.58216568, -0.49043875],\n       [-2.84080901,  1.21976092, -2.29653591,  1.38693479,  0.89124752],\n       [ 4.05835427,  0.18826113, -1.47195916, -2.08012185, -0.38826235]])</pre> In\u00a0[41]: Copied! <pre>pca.get_covariance()\n</pre> pca.get_covariance() Out[41]: <pre>array([[10.1       ,  1.44444444,  0.14444444, -1.02222222,  2.54444444],\n       [ 1.44444444,  5.77777778,  2.33333333, -1.55555556, -0.22222222],\n       [ 0.14444444,  2.33333333,  9.12222222, -0.71111111, -1.74444444],\n       [-1.02222222, -1.55555556, -0.71111111,  4.4       , -0.8       ],\n       [ 2.54444444, -0.22222222, -1.74444444, -0.8       ,  7.43333333]])</pre> In\u00a0[42]: Copied! <pre>pca = PCA(n_components=2)\npca.fit_transform(mat)\n</pre> pca = PCA(n_components=2) pca.fit_transform(mat) Out[42]: <pre>array([[ 6.02891327, -3.03148483],\n       [ 0.36293328,  6.7467456 ],\n       [-3.19525013,  2.6727019 ],\n       [-4.94628015, -2.12873178],\n       [-2.32548828, -4.23812621],\n       [-0.99486081, -3.10456726],\n       [ 2.05998852,  2.21360015],\n       [ 1.79249902, -0.53815962],\n       [-2.84080901,  1.21976092],\n       [ 4.05835427,  0.18826113]])</pre> In\u00a0[43]: Copied! <pre>test_mat_a = pca.fit_transform(mat)\n</pre> test_mat_a = pca.fit_transform(mat) In\u00a0[44]: Copied! <pre>test_mat_b = pca_scratch(mat, n_components=3)\n</pre> test_mat_b = pca_scratch(mat, n_components=3) In\u00a0[45]: Copied! <pre>test_mat_b\n</pre> test_mat_b Out[45]: <pre>(array([[ 1.34406967,  0.79893634, -0.21772407],\n        [-1.41021551,  0.85653045, -0.26744592],\n        [-0.78097635, -0.15741156,  0.63733731],\n        [-0.26405395, -1.12388807, -0.40691056],\n        [ 0.79188142, -0.71150037,  0.81230304],\n        [ 0.38443738, -0.78581598, -0.53497207],\n        [-0.01859968,  0.87595551,  0.70023784],\n        [ 0.30756853,  0.11254948,  0.08408886],\n        [-0.65727198, -0.54219599, -0.10409917],\n        [ 0.30316048,  0.67684018, -0.70281526]]),\n array([0.65206722, 0.59062673, 0.29321415, 0.26655314, 0.15235349]),\n array([[ 0.27024738, -0.26995178, -0.65105981,  0.00982356,  0.65583307],\n        [ 0.59933572,  0.50482879,  0.42251158, -0.24499416,  0.38393542],\n        [-0.6004416 , -0.16705   ,  0.4575947 , -0.0313304 ,  0.63339536]]),\n array([[ 0.42065806,  0.07369615,  0.00685546, -0.04346183,  0.13314728],\n        [ 0.07369615,  0.36111111,  0.13565891, -0.08101852, -0.01424501],\n        [ 0.00685546,  0.13565891,  0.49335977, -0.03445306, -0.10402173],\n        [-0.04346183, -0.08101852, -0.03445306,  0.19097222, -0.04273504],\n        [ 0.13314728, -0.01424501, -0.10402173, -0.04273504,  0.48871357]]))</pre> In\u00a0[46]: Copied! <pre>np.dot(test_mat_b[0][:,0], test_mat_b[0][:,2].T).round(5)\n</pre> np.dot(test_mat_b[0][:,0], test_mat_b[0][:,2].T).round(5) Out[46]: <pre>-0.0</pre> In\u00a0[47]: Copied! <pre>np.dot(test_mat_a[:,0], test_mat_a[:,1].T).round(5)\n</pre> np.dot(test_mat_a[:,0], test_mat_a[:,1].T).round(5) Out[47]: <pre>0.0</pre> In\u00a0[48]: Copied! <pre>pca.get_covariance()\n</pre> pca.get_covariance() Out[48]: <pre>array([[ 9.80302657,  1.53051848,  0.20514549, -1.33168657,  2.89697164],\n       [ 1.53051848,  6.07639082,  2.33658729, -0.7720984 , -0.17790607],\n       [ 0.20514549,  2.33658729,  9.03076514, -0.82433389, -1.90304015],\n       [-1.33168657, -0.7720984 , -0.82433389,  4.95655128, -0.38259929],\n       [ 2.89697164, -0.17790607, -1.90304015, -0.38259929,  6.96659952]])</pre> In\u00a0[49]: Copied! <pre>test_mat_a, test_mat_b[0]\n</pre> test_mat_a, test_mat_b[0] Out[49]: <pre>(array([[ 6.02891327, -3.03148483],\n        [ 0.36293328,  6.7467456 ],\n        [-3.19525013,  2.6727019 ],\n        [-4.94628015, -2.12873178],\n        [-2.32548828, -4.23812621],\n        [-0.99486081, -3.10456726],\n        [ 2.05998852,  2.21360015],\n        [ 1.79249902, -0.53815962],\n        [-2.84080901,  1.21976092],\n        [ 4.05835427,  0.18826113]]),\n array([[ 1.34406967,  0.79893634, -0.21772407],\n        [-1.41021551,  0.85653045, -0.26744592],\n        [-0.78097635, -0.15741156,  0.63733731],\n        [-0.26405395, -1.12388807, -0.40691056],\n        [ 0.79188142, -0.71150037,  0.81230304],\n        [ 0.38443738, -0.78581598, -0.53497207],\n        [-0.01859968,  0.87595551,  0.70023784],\n        [ 0.30756853,  0.11254948,  0.08408886],\n        [-0.65727198, -0.54219599, -0.10409917],\n        [ 0.30316048,  0.67684018, -0.70281526]]))</pre> In\u00a0[50]: Copied! <pre>test_mat_a.round(2), test_mat_b[0].round(2)\n</pre> test_mat_a.round(2), test_mat_b[0].round(2) Out[50]: <pre>(array([[ 6.03, -3.03],\n        [ 0.36,  6.75],\n        [-3.2 ,  2.67],\n        [-4.95, -2.13],\n        [-2.33, -4.24],\n        [-0.99, -3.1 ],\n        [ 2.06,  2.21],\n        [ 1.79, -0.54],\n        [-2.84,  1.22],\n        [ 4.06,  0.19]]),\n array([[ 1.34,  0.8 , -0.22],\n        [-1.41,  0.86, -0.27],\n        [-0.78, -0.16,  0.64],\n        [-0.26, -1.12, -0.41],\n        [ 0.79, -0.71,  0.81],\n        [ 0.38, -0.79, -0.53],\n        [-0.02,  0.88,  0.7 ],\n        [ 0.31,  0.11,  0.08],\n        [-0.66, -0.54, -0.1 ],\n        [ 0.3 ,  0.68, -0.7 ]]))</pre> <p>The values are not exactly matching with the <code>sklearn.decomposition.PCA</code> class as the solvers are different. However in both the cases the test of principal component i.e. dot product = 0 is validated</p> In\u00a0[51]: Copied! <pre>import numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n</pre> import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer In\u00a0[52]: Copied! <pre>mat = np.random.randint(1, high = 10, size=(4, 3))\nprint(mat)\n</pre> mat = np.random.randint(1, high = 10, size=(4, 3)) print(mat) In\u00a0[53]: Copied! <pre>sc = StandardScaler()\nnm = sc.fit_transform(mat)\n</pre> sc = StandardScaler() nm = sc.fit_transform(mat) In\u00a0[54]: Copied! <pre>nm\n</pre> nm In\u00a0[55]: Copied! <pre>colMean = np.mean(mat, axis=0)\nmat - colMean\n</pre> colMean = np.mean(mat, axis=0) mat - colMean In\u00a0[56]: Copied! <pre>colStd = np.std(mat, axis = 0)\n</pre> colStd = np.std(mat, axis = 0) In\u00a0[57]: Copied! <pre>(mat - colMean)/colStd\n</pre> (mat - colMean)/colStd In\u00a0[58]: Copied! <pre>((mat - colMean)/colStd == nm).all()\n</pre> ((mat - colMean)/colStd == nm).all() In\u00a0[59]: Copied! <pre>minMax = MinMaxScaler(feature_range=(0, 5))\nnm = minMax.fit_transform(mat)\n</pre> minMax = MinMaxScaler(feature_range=(0, 5)) nm = minMax.fit_transform(mat) In\u00a0[60]: Copied! <pre>nm\n</pre> nm In\u00a0[61]: Copied! <pre>colMax = np.max(mat, axis = 0)\ncolMin = np.min(mat, axis = 0)\ncolMax, colMin\n</pre> colMax = np.max(mat, axis = 0) colMin = np.min(mat, axis = 0) colMax, colMin <p>$$ \\frac{x - min}{max - min} \\times (max_{new} - min_{new}) + min_{new} $$</p> In\u00a0[62]: Copied! <pre>(mat - colMin)/(colMax - colMin) * 5\n</pre> (mat - colMin)/(colMax - colMin) * 5 In\u00a0[63]: Copied! <pre>(np.round(nm, 4) == np.round((mat - colMin)/(colMax - colMin) * 5, 4)).all()\n</pre> (np.round(nm, 4) == np.round((mat - colMin)/(colMax - colMin) * 5, 4)).all() <p>$$ L2_{Norm} = \\sqrt{\\sum_i^N {x_i}^2} $$</p> In\u00a0[64]: Copied! <pre>norm = Normalizer()\nnm = norm.fit_transform(mat)\n</pre> norm = Normalizer() nm = norm.fit_transform(mat) In\u00a0[65]: Copied! <pre>nm\n</pre> nm In\u00a0[66]: Copied! <pre>np.divide(mat, np.sqrt(np.sum(np.square(mat), axis=1)).reshape(4, 1))\n</pre> np.divide(mat, np.sqrt(np.sum(np.square(mat), axis=1)).reshape(4, 1)) In\u00a0[67]: Copied! <pre>(nm == np.divide(mat, np.sqrt(np.sum(np.square(mat), axis=1)).reshape(4, 1))).all()\n</pre> (nm == np.divide(mat, np.sqrt(np.sum(np.square(mat), axis=1)).reshape(4, 1))).all() In\u00a0[68]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[69]: Copied! <pre>from sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.datasets import make_regression, make_classification\n</pre> from sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.datasets import make_regression, make_classification In\u00a0[70]: Copied! <pre>data = make_regression(n_features=2)\n</pre> data = make_regression(n_features=2) In\u00a0[71]: Copied! <pre>data\n</pre> data In\u00a0[72]: Copied! <pre>lr = LinearRegression()\n</pre> lr = LinearRegression() In\u00a0[73]: Copied! <pre>lr.fit(*data)\n</pre> lr.fit(*data) In\u00a0[74]: Copied! <pre>preds = lr.predict(data[0])\n</pre> preds = lr.predict(data[0]) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[75]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n</pre> from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[76]: Copied! <pre>from sklearn.cluster import KMeans, DBSCAN\n</pre> from sklearn.cluster import KMeans, DBSCAN In\u00a0[77]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n</pre> from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier In\u00a0[78]: Copied! <pre>from sklearn.linear_model import LinearRegression, LogisticRegression\n</pre> from sklearn.linear_model import LinearRegression, LogisticRegression In\u00a0[79]: Copied! <pre>lr = LinearRegression()\n</pre> lr = LinearRegression() In\u00a0[80]: Copied! <pre>lgr = LogisticRegression()\n</pre> lgr = LogisticRegression() In\u00a0[81]: Copied! <pre>lr.fit(mat, np.random.random(4))\n</pre> lr.fit(mat, np.random.random(4)) In\u00a0[82]: Copied! <pre>lr.predict(np.array([[3, 5, 6]]))\n</pre> lr.predict(np.array([[3, 5, 6]])) In\u00a0[83]: Copied! <pre>rf = RandomForestClassifier()\nrf.fit(mat, np.array([0, 1, 0, 1]))\n</pre> rf = RandomForestClassifier() rf.fit(mat, np.array([0, 1, 0, 1])) In\u00a0[84]: Copied! <pre>dbscan = DBSCAN()\n</pre> dbscan = DBSCAN() In\u00a0[85]: Copied! <pre>dbscan.fit(mat)\n</pre> dbscan.fit(mat) In\u00a0[86]: Copied! <pre>dbscan.labels_\n</pre> dbscan.labels_ In\u00a0[87]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n</pre> from sklearn.preprocessing import StandardScaler In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Data%20Science/a.%20Statistics/#statistics-primer","title":"Statistics Primer\u00b6","text":"<p>Statistics is a form of mathematical analysis that uses different mathematical tools on a given data set to describe and make an inference out of it.</p> <p></p>"},{"location":"Data%20Science/a.%20Statistics/#descriptive-statistics","title":"Descriptive Statistics\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#measures-of-central-tendency","title":"Measures of Central Tendency\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#mean","title":"Mean\u00b6","text":"<p>Average value of all the numbers.</p> <p>Mean is affected by the presence of large values or outliers in the dataset and is mostly used for Normal or Gaussian Distribution.</p>"},{"location":"Data%20Science/a.%20Statistics/#median","title":"Median\u00b6","text":"<p>Middle value of the data set</p> <p>Median is not affected by the presence of large values or outliers in the dataset and is mostly for Skewed distributions.</p>"},{"location":"Data%20Science/a.%20Statistics/#mode","title":"Mode\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#measures-of-variability","title":"Measures of Variability\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#law-of-large-numbers","title":"Law of Large Numbers\u00b6","text":"<p>In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.</p> <p>The LLN only applies to the average. Therefore, while $${\\lim_{n \\to \\infty} \\sum _{i=1}^{n}{\\frac {X_{i}}{n}}={\\overline {X}}}$$ other formulas that look similar are not verified, such as the raw deviation from \"theoretical results\": $$\\sum _{i=1}^{n}X_{i}-n\\times {\\overline {X}}$$ not only does it not converge toward zero as $n$ increases, but it tends to increase in absolute value as $n$ increases.</p>"},{"location":"Data%20Science/a.%20Statistics/#central-limit-theorem","title":"Central Limit Theorem\u00b6","text":"<p>The central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.</p> <p></p> <p>Classical CLT Let ${X_{1},\\dots ,X_{n}}$ be a sequence of random samples \u2014 that is, a sequence of i.i.d. random variables drawn from a distribution of expected value given by $\\mu$ and finite variance given by $\\sigma^2$. Suppose we are interested in the sample average $$ \\bar{X_{n}} \\equiv \\frac{X_{1}+\\cdots +X_{n}}{n} $$ of the first $n$ samples. By the law of large numbers, the sample averages converge almost surely (and therefore also converge in probability) to the expected value $\\mu$ as $n \\rightarrow \\infty$.</p>"},{"location":"Data%20Science/a.%20Statistics/#co-variance","title":"Co-variance:\u00b6","text":"<p>Covariance measures the direction of the relationship between two variables. A positive covariance means that both variables tend to be high or low at the same time. A negative covariance means that when one variable is high, the other tends to be low. $$ \\begin{align} cov_{x, y} = &amp; \\frac{\\sum(x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{N-1} &amp; [-\\infty, \\infty] \\end{align} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#correlation","title":"Correlation:\u00b6","text":"<p>$$ \\begin{align} r_{xy} = \\rho_{X,Y}={corr}(X,Y)= &amp; \\frac{{cov}(X,Y)}{\\sigma_{X}\\sigma _{Y}} =\\frac{E[(X-\\mu _{X})(Y-\\mu _{Y})]}{\\sigma _{X}\\sigma _{Y}}&amp; {\\text{if   } \\sigma _{X}\\sigma _{Y}&gt;0.}&amp;&amp; [-1, +1] \\end{align} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#what-is-the-difference-between-covariance-and-correlation","title":"What Is the Difference Between Covariance and Correlation?\u00b6","text":"<p>Covariance measures the direction of a relationship between two variables, while correlation measures the strength of that relationship. Both correlation and covariance are positive when the variables move in the same direction and negative when they move in opposite directions. However, a correlation coefficient must always be between $-1$ and $+1$, with extreme values indicating a strong relationship. </p>"},{"location":"Data%20Science/a.%20Statistics/#inter-quartile-range","title":"Inter Quartile Range\u00b6","text":"<p>$$ IQR = Q3 \u2212  Q1 $$</p> <p>Algorithm The IQR of a set of values is calculated as the difference between the upper and lower quartiles, Q3 and Q1. Each quartile is a median calculated as follows.</p> <p>Given an even $2n$ or odd $(2n+1)$ number of values:</p> <ul> <li>first quartile Q1 = median of the n smallest values</li> <li>third quartile Q3 = median of the n largest values</li> <li>The second quartile Q2 is the same as the ordinary median.</li> </ul> <p>Examples</p> <p>The following table has 13 rows, and follows the rules for the odd number of entries.</p>"},{"location":"Data%20Science/a.%20Statistics/#box-plot","title":"Box-Plot\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#normal-distribution","title":"Normal Distribution\u00b6","text":"<p>Normal Distribution has the following characteristics.</p> <ul> <li><p>In Normal Distribution we have mean = median = mode.</p> </li> <li><p>Normal Distribution is symmetric about the center (mean).</p> </li> <li><p>It has 50% of the values above the mean and 50% of the values below the mean. It is also obvious from the graphical representation above.</p> </li> <li><p>The total area under the curve shown above is 1.</p> </li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#standard-normal-distribution","title":"Standard Normal Distribution\u00b6","text":"<p>Standard Normal Distribution is a distribution having a mean of zero and standard deviation of one. We can convert any ordinary Normal Distribution to Standard Normal Distribution, and this process is called Standarization. It helps us find out how many standard deviations a particular observation is from the mean. $$ z_i = \\frac{x_i - \\mu}{\\sigma} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#population","title":"Population\u00b6","text":"<p>Population consists of similar elements or individuals which are of interest for a particular experiment or result.</p>"},{"location":"Data%20Science/a.%20Statistics/#sample","title":"Sample\u00b6","text":"<p>A sample consists of elements or individuals drawn from a population using some procedure.</p>"},{"location":"Data%20Science/a.%20Statistics/#hypothesis-testing","title":"Hypothesis testing\u00b6","text":"<p>It involves using sample data to answer some questions regarding to the population. These questions can be</p> <ul> <li>Inferring the mean of the population using the sample.</li> <li>Does one population have the same or different mean from the other population?</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#confidence-interval","title":"Confidence interval\u00b6","text":"<p>The process of estimating the population parameters like mean and standard deviation using the sample. Confidence intervals are meant to create a range of values in which the population parameter is likely to fall.</p>"},{"location":"Data%20Science/a.%20Statistics/#bayes-theorem","title":"Bayes' Theorem\u00b6","text":"<p>Bayes\u2019 theorem (alternatively Bayes\u2019 law or Bayes\u2019 rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event.</p> <p>Bayes\u2019 theorem is stated as seen below.</p> <p>$$ P(A\u2223B) = \\frac{P(B\u2223A) \\cdot P(A)}{P(B)} $$</p> <p>$P(A\u2223B) =$ is the conditional probability of A given B has occurred. It is called the Posterior Probability.</p> <p>$P(B\u2223A) =$ is the conditional probability of B given A has occurred. It is called the Likelihood.</p> <p>$P(A) =$ is the probability of A. It is called the Prior Probability.</p> <p>$P(B) =$ is the probability of B. It is called the Evidence.</p>"},{"location":"Data%20Science/a.%20Statistics/#hypothesis-testing","title":"Hypothesis Testing\u00b6","text":"<p>Hypothesis testing is a statistical analysis that uses sample data to assess two mutually exclusive theories about the properties of a population. We make a hypothesis about the dataset at hand and ask different questions. Hypothesis testing helps us to find the likelihood of answers to those questions that we ask.</p>"},{"location":"Data%20Science/a.%20Statistics/#approaches-to-hypothesis-testing","title":"Approaches to hypothesis testing\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#p-value-approach","title":"P-value approach\u00b6","text":"<p>In these types of tests, the hypothesis tests return a $p_{value}$, which is compared against a chosen level of significance $\\alpha$ to reject or fail to reject the null hypothesis. For-example:</p> <ul> <li>If $p_{value} \u2264 \\alpha$, reject the null-hypothesis.</li> <li>If $p_{value} &gt; \\alpha$, don\u2019t reject the null hypothesis.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#critical-value-approach","title":"Critical-value approach\u00b6","text":"<p>In these types of test, the hypothesis tests return a test statistic from the data under consideration which can be interpreted in the context of critical values. A critical value is a value from the sampling distribution of the test statistic after which point the null hypothesis can be rejected i.e</p> <ul> <li>If |Test Statistic| &lt; Critical Value, don\u2019t reject the null hypothesis.</li> <li>If |Test Statistic| \u2265 Critical Value, reject the null hypothesis.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#one-tailed-test","title":"One-tailed Test\u00b6","text":"<p>The one-tailed test has a critical value on the right side of the distribution for non-symmetrical distributions. The test statistic is compared to the calculated critical-value and the results are inferred as mentioned in the critical value approach.</p>"},{"location":"Data%20Science/a.%20Statistics/#two-tailed-test","title":"Two-tailed Test\u00b6","text":"<p>A two-tailed test has two critical values. There is one on each side of the distribution, which is often assumed to be symmetrical. When using a two-tailed test, a significance level (or $\\alpha$) is used in the calculation of the critical values must be divided by two. This would be split to give two alpha values of 2.5% on either side of the distribution with an acceptance area in the middle of the distribution of 95%. Rules for acceptance or rejection of the hypothesis will change as seen below.</p> <ul> <li>If Lower Critical Value \u2264 Test Statistic \u2265 Upper Critical Value, don\u2019t reject the null hypothesis.</li> <li>If Test Statistic &lt; Lower Critical Value OR Test Statistic &gt; Upper Critical Value, then reject the null hypothesis.</li> </ul> <p>If the distribution of the test statistic is symmetric around a mean of zero, then we can shortcut the check by comparing the absolute (positive) value of the test statistic to the upper critical value.</p> <ul> <li>If |Test Statistic| \u2264 Upper Critical Value then don\u2019t reject the null hypothesis.</li> <li>If |Test Statistic| &gt; Upper Critical Value then reject the null hypothesis.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#t-test","title":"T-test\u00b6","text":"<p>It is a statistical hypothesis test where two independent data samples known to have a Gaussian Distribution, have the same Gaussian Distribution.</p> <ul> <li>Null hypothesis: There is no difference between the sample means.</li> <li>Alternative hypothesis: There is some difference between the sample means.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#data-variables","title":"Data Variables\u00b6","text":"<ul> <li>Input Variables (Attributes/Predictors/Features/Independent Variables) $\\rightarrow X$</li> <li>Output Variables (Labels/Response Variables/Dependent Variables) $\\rightarrow Y$</li> </ul> <p>Variables:</p> <ul> <li>Quantitative (or Numeric) e.g. a person\u2019s age, height, or income, the value of a house, and the price of a stock.</li> <li>Qualitative (or Categorical) a person\u2019s gender (male or female), the brand of a product (brand A, B, or C), whether a person defaults on a debt (yes or no).</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#desiderata-requirements-for-learning-algorithms","title":"Desiderata (Requirements) for learning algorithms\u00b6","text":"<ul> <li>Predictive efficiency: e.g., how small is a typical test MSE?</li> <li>Computational efficiency and scalability $\\text{Big-}O$ Notation. Two main aspects:<ul> <li>time</li> <li>space</li> </ul> </li> <li>Interpretability</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#problem-formulation-type","title":"Problem Formulation Type\u00b6","text":"<ul> <li>Regression $\\rightarrow$ Problems with a quantitative label (output)</li> <li>Classification $\\rightarrow$ Problems with a qualitative label (output)</li> </ul> <p>More generally, suppose that we observe a quantitative label $Y$, and $p$ different attributes, $X_1, X_2, \\dots , X_p$. We assume that there is a relationship between $Y$ and $X = (X_1, X_2, \\dots , X_p)$, which, in the case of regression, can be written in the very general form:</p> <p>$$ \\begin{equation} Y = f(X) + \\epsilon \\end{equation} $$</p> <p>Here $f$ is some fixed but unknown function of $X_1, X_2, \\dots , X_p$ and $\\epsilon$ is a random error term, which is independent of $X$ and has mean $0.$ There are similar assumptions in the case of classification problems: a function $f(X)$ is distorted by random noise.</p>"},{"location":"Data%20Science/a.%20Statistics/#assumptions-of-linear-regression","title":"Assumptions of Linear Regression\u00b6","text":"<p>Linear regression relies on certain assumptions, including:</p> <ul> <li>Linearity: The relationship between the dependent and independent variables is assumed to be linear.</li> <li>Independence: The observations are assumed to be independent of each other.</li> <li>Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the independent variables.</li> <li>Normality: The errors follow a normal distribution.</li> <li>No Auto-Correleation: The error terms should be independent of each other.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#linear-regression","title":"Linear Regression\u00b6","text":"<p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It is called \"linear\" because the model assumes a linear relationship between the dependent and independent variables.</p>"},{"location":"Data%20Science/a.%20Statistics/#measuring-the-quality-of-fit","title":"Measuring the quality of fit\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#regression","title":"Regression\u00b6","text":"<ul> <li><p>$MSE(SSR) = \\frac{1}{n}\\sum_{i = 1}^{n}(y_i - \\hat{f}(x_i))^2$, where $\\hat{f}(x_i)$ is the prediction that $\\hat{f}$ gives for the $i^{th}$ observation.</p> </li> <li><p>$RMSE = \\sqrt{MSE}$</p> </li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#r-squared-r2-explains-the-amount-of-variance-captured-by-the-model-r2-ranges-from-0-to-1-where-1-indicates-a-perfect-fit","title":"R-squared (R\u00b2): Explains the amount of variance captured by the model. R\u00b2 ranges from 0 to 1, where 1 indicates a perfect fit.\u00b6","text":"<p>It represents the proportion of the variance in the dependent variable that is explained by the independent variables. $$R^2 = 1 - \\left(\\frac{SSR}{TSS}\\right)$$</p> <ul> <li>$SSR$ is the Sum of Squares Residual, which represents the sum of squared residuals (differences between observed and predicted values)$\\sum_{i} (y_i - y'_i)^2$.</li> <li>$TSS$ is the Total Sum of Squares, which represents the sum of squared differences between observed values and the mean of the dependent variable. $\\sum_{i} (y_i - \\bar{y})^2$</li> </ul> <pre><code>Where:  </code></pre> <ul> <li>$R^2$ is the R-squared value.</li> <li>$n$ is the sample size.</li> <li>$p$ is the number of independent variables in the model.</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#adjusted-r-squared-it-adjusts-r2-by-considering-the-number-of-independent-variables-and-the-sample-size-providing-a-better-measure-of-model-fit-when-comparing-different-models","title":"Adjusted R-squared: It adjusts R\u00b2 by considering the number of independent variables and the sample size, providing a better measure of model fit when comparing different models.\u00b6","text":"<p>$$ Adjusted\\ R^2 = 1 - \\left(\\frac{1 - R^2}{n - p - 1}\\right) $$</p>"},{"location":"Data%20Science/a.%20Statistics/#hypothesis-in-linear-regression","title":"Hypothesis in Linear Regression\u00b6","text":"<p>Once you have fitted a straight line on the data, you need to ask,</p> <ul> <li>Is this straight line a significant fit for the data? OR</li> <li>Is the beta coefficient explain the variance in the data plotted?</li> </ul> <p>The Null and Alternate hypotheses in this case are:</p> <ul> <li>H0: B1 = 0</li> <li>HA: B1 \u2260 0</li> </ul> <p>To test this hypothesis we use a t-test, test statistics for the beta coefficient is given by,</p>"},{"location":"Data%20Science/a.%20Statistics/#assessing-the-model-fit","title":"Assessing the model fit:\u00b6","text":"<p>Some other parameters to assess a model are:</p> <p>t statistic: It is used to determine the p-value and hence, helps in determining whether the coefficient is significant or not  $\\to t = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}$ F statistic: It is used to assess whether the overall model fit is significant or not. Generally, the higher the value of the F-statistic, the more significant a model turns out to be.</p>"},{"location":"Data%20Science/a.%20Statistics/#homoscedasticity-and-heteroscedasticity-are-terms-used-to-describe-the-variability-of-the-errors-residuals-in-a-regression-model-across-different-levels-of-the-independent-variables","title":"Homoscedasticity and heteroscedasticity are terms used to describe the variability of the errors (residuals) in a regression model across different levels of the independent variables.\u00b6","text":"<p>1. Homoscedasticity: Homoscedasticity, also known as constant variance, refers to the situation where the variability of the errors is consistent across all levels of the independent variables. In other words, the spread of the residuals is the same regardless of the values of the predictors.</p> <p>Homoscedasticity is an assumption of linear regression. When this assumption is met, it implies that the model's errors have a consistent level of precision and do not systematically increase or decrease as the predicted values change. This allows for reliable inference and accurate confidence intervals.</p> <p>Visually, homoscedasticity can be assessed by plotting the residuals against the predicted values. If the points in the plot are randomly scattered around a horizontal line with a relatively constant spread, it indicates homoscedasticity.</p> <p>2. Heteroscedasticity: Heteroscedasticity, or non-constant variance, occurs when the variability of the errors differs across different levels of the independent variables. In this case, the spread of the residuals tends to change as the predicted values change or as the values of the predictors vary.</p> <p>Heteroscedasticity violates the assumption of homoscedasticity in linear regression. It can cause problems in the regression analysis, such as biased coefficient estimates, invalid hypothesis tests, and unreliable confidence intervals. The standard errors of the coefficients may be incorrect, leading to incorrect p-values and wrong conclusions about the significance of the predictors.</p> <p>To detect heteroscedasticity, you can plot the residuals against the predicted values or against the independent variables. If the plot shows a pattern where the spread of the residuals widens or narrows systematically, it suggests heteroscedasticity.</p> <p>Dealing with Heteroscedasticity: If heteroscedasticity is detected in a regression analysis, there are a few approaches to address it:</p> <ul> <li>Transforming Variables: Applying transformations to the variables, such as logarithmic or square root transformations, can help stabilize the variance.</li> <li>Weighted Least Squares: Assigning different weights to the observations based on the estimated variability can be used to give more importance to the observations with smaller variances.</li> <li>Robust Standard Errors: Using robust standard errors can provide valid inference even in the presence of heteroscedasticity. Robust standard errors adjust for heteroscedasticity without transforming the variables or changing the estimation method.</li> </ul> <p>It is important to identify and address heteroscedasticity in regression analysis to ensure the validity and reliability of the model's results and interpretations.</p>"},{"location":"Data%20Science/a.%20Statistics/#classification","title":"Classification\u00b6","text":"<ul> <li>Error Rate = $\\frac{1}{n}\\sum_{i = 1}^{n}I(y_i \\neq \\hat{y}_i)$, Here $\\hat{y}_i = \\hat{f}(x_i)$ is the predicted label for the $i^{th}$ observation using $\\hat{f}$, and</li> </ul> <p>$$ \\begin{equation*} I(y_i \\neq \\hat{y}_i) = \\begin{cases}     &amp; 1 &amp; \\text{ if $(y_i \\neq \\hat{y}_i)$} \\\\     &amp; 0 &amp; \\text{ if $(y_i = \\hat{y}_i)$} \\end{cases} \\end{equation*} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#pca-principal-component-analysis","title":"PCA (Principal Component Analysis)\u00b6","text":"<p>SVD Eigen Values and Eigen-Vectors</p>"},{"location":"Data%20Science/a.%20Statistics/#all-components","title":"All Components\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#n-components","title":"N-Components\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#scaling","title":"Scaling\u00b6","text":"<ul> <li>Min-Max</li> <li>Standardization</li> <li>Normalization (Row Data using L2 Norm)</li> </ul>"},{"location":"Data%20Science/a.%20Statistics/#manual-implementation-standardizer","title":"Manual Implementation Standardizer\u00b6","text":"<p>$$ z_i = \\frac{x_i - \\mu}{\\sigma} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#manual-implementation-scaling","title":"Manual Implementation Scaling\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#manual-implementation-normalization-l2","title":"Manual Implementation Normalization $(L2)$\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#data-split","title":"Data Split\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#supervised-algorithms","title":"Supervised Algorithms\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#regression","title":"Regression\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#linear","title":"Linear\u00b6","text":"<p>Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It\u2019s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:</p> <ul> <li>Simple regression:<ul> <li>Simple linear regression uses traditional slope-intercept form, where $m$ and $b$ are the variables our algorithm will try to \u201clearn\u201d to produce the most accurate predictions. $x$ represents our input data and $y$ represents our prediction.</li> </ul> </li> </ul> <p>$$y=mx+b$$</p> <ul> <li>Multivariable regression:<ul> <li>A more complex, multi-variable linear equation might look like this, where $w$ represents the coefficients, or weights, our model will try to learn. $$ f(x,y,z)=w_1x+w_2y+w_3z $$</li> </ul> </li> </ul> <p>The variables $x,y,z$ represent the attributes, or distinct pieces of information, we have about each observation.</p> <p></p>"},{"location":"Data%20Science/a.%20Statistics/#loss","title":"Loss\u00b6","text":"<p>$$ MSE =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (w x_i + b))^2 $$</p> <p>To minimize MSE we use Gradient Descent to calculate the gradient of our cost function. Gradient descent consists of looking at the error that our weight currently gives us, using the derivative of the cost function to find the gradient (The slope of the cost function using our current weight), and then changing our weight to move in the direction opposite of the gradient (steepest ascent). We need to move in the opposite direction of the gradient since the gradient points up the slope instead of down it, so we move in the opposite direction to try to decrease our error. $$ \\begin{align} J'(w,b) =   \\begin{bmatrix}     \\frac{df}{dw}\\\\     \\frac{df}{db}\\\\   \\end{bmatrix} &amp;=   \\begin{bmatrix}      \\frac{1}{N} \\sum -2x_i(y_i - (wx_i + b)) \\\\      \\frac{1}{N} \\sum -2(y_i - (wx_i + b)) \\\\   \\end{bmatrix} \\end{align} $$</p> <p>$$ \\begin{align} &amp; W = W - \\alpha \\times dW &amp; dW = \\frac{df}{dw} \\\\ &amp; b = b - \\alpha \\times db &amp; db = \\frac{df}{db} \\\\ \\end{align} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#logistic-regression","title":"Logistic Regression\u00b6","text":"<p>Classification model where we fit a linear regression model to the input features, but we pass the predicted continuous output to a sigmoid (or softmax in case of multiple classes) function , that gives us the probabilty of each class.</p> <p>Types of logistic regression</p> <ul> <li>Binary (Pass/Fail)</li> <li>Multi (Cats, Dogs, Sheep)</li> <li>Ordinal (Low, Medium, High)</li> </ul> <p>Sigmoid $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$ Softmax $$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}} $$ </p>"},{"location":"Data%20Science/a.%20Statistics/#decision-trees","title":"Decision Trees\u00b6","text":"<p>Gini Index $$ \\begin{align} &amp; G = \\sum p_{mk} \\times (1-p_{mk}) &amp; \\text{ if } p_{mk} = 0 \\implies (1- p_{mk}) = 1 \\end{align} $$ Entropy $$ E = -\\sum p_{mk} \\times \\log(p_{mk}) $$ Information Gain $$ IG = E(parent) - [\\textit{weighted average}] * E(children) $$</p>"},{"location":"Data%20Science/a.%20Statistics/#bagging","title":"Bagging\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#random-forest","title":"Random Forest\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#boosting","title":"Boosting\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#time-series","title":"Time Series\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#arima","title":"ARIMA\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#unsupervised-algorithms","title":"Unsupervised Algorithms\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#k-nearest-neighbours","title":"K-Nearest Neighbours\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#k-means-clustering","title":"K-Means Clustering\u00b6","text":"<p>$$ Eucledian(x_i, y_i) = \\sqrt{\\sum (x_i - y_i)^2} $$</p>"},{"location":"Data%20Science/a.%20Statistics/#dbscan","title":"DBSCAN\u00b6","text":""},{"location":"Data%20Science/a.%20Statistics/#hierarchical-clustering","title":"Hierarchical Clustering\u00b6","text":""},{"location":"Data%20Science/b.%20Problem%20Formulation/","title":"Data Variables","text":""},{"location":"Data%20Science/b.%20Problem%20Formulation/#data-variables","title":"Data Variables\u00b6","text":"<ul> <li>Input Variables (Attributes/Predictors/Features/Independent Variables) $\\rightarrow X$</li> <li>Output Variables (Labels/Response Variables/Dependent Variables) $\\rightarrow Y$</li> </ul> <p>Variables:</p> <ul> <li>Quantitative (or Numeric) e.g. a person\u2019s age, height, or income, the value of a house, and the price of a stock.</li> <li>Qualitative (or Categorical) a person\u2019s gender (male or female), the brand of a product (brand A, B, or C), whether a person defaults on a debt (yes or no).</li> </ul>"},{"location":"Data%20Science/b.%20Problem%20Formulation/#desiderata-requirements-for-learning-algorithms","title":"Desiderata (Requirements) for learning algorithms\u00b6","text":"<ul> <li>Predictive efficiency: e.g., how small is a typical test MSE?</li> <li>Computational efficiency and scalability $\\text{Big-}O$ Notation. Two main aspects:<ul> <li>time</li> <li>space</li> </ul> </li> <li>Interpretability</li> </ul>"},{"location":"Data%20Science/b.%20Problem%20Formulation/#problem-formulation-type","title":"Problem Formulation Type\u00b6","text":"<ul> <li>Regression $\\rightarrow$ Problems with a quantitative label (output)</li> <li>Classification $\\rightarrow$ Problems with a qualitative label (output)</li> </ul> <p>More generally, suppose that we observe a quantitative label $Y$, and $p$ different attributes, $X_1, X_2, \\dots , X_p$. We assume that there is a relationship between $Y$ and $X = (X_1, X_2, \\dots , X_p)$, which, in the case of regression, can be written in the very general form:</p> <p>$$ \\begin{equation} Y = f(X) + \\epsilon \\end{equation} $$</p> <p>Here $f$ is some fixed but unknown function of $X_1, X_2, \\dots , X_p$ and $\\epsilon$ is a random error term, which is independent of $X$ and has mean $0.$ There are similar assumptions in the case of classification problems: a function $f(X)$ is distorted by random noise.</p>"},{"location":"Data%20Science/sample/","title":"Changing the fonts","text":"<p>Material for MkDocs makes it easy to change the typeface of your project documentation, as it directly integrates with Google Fonts. Alternatively, fonts can be custom-loaded if self-hosting is preferred for data privacy reasons or another destination should be used.</p>"},{"location":"Data%20Science/sample/#configuration","title":"Configuration","text":""},{"location":"Data%20Science/sample/#regular-font","title":"Regular font","text":"<p>The regular font is used for all body copy, headlines, and essentially everything that does not need to be monospaced. It can be set to any valid Google Font via <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  font:\n    text: Roboto\n</code></pre> <p>The typeface will be loaded in 300, 400, 400i and 700.</p>"},{"location":"Data%20Science/sample/#monospaced-font","title":"Monospaced font","text":"<p>The monospaced font is used for code blocks and can be configured separately. Just like the regular font, it can be set to any valid Google Font via <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  font:\n    code: Roboto Mono\n</code></pre> <p>The typeface will be loaded in 400.</p>"},{"location":"Data%20Science/sample/#autoloading","title":"Autoloading","text":"<p>If you want to prevent typefaces from being loaded from Google Fonts, e.g. to adhere to data privacy regulations, and fall back to system fonts, add the following lines to <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  font: false\n</code></pre> <p>Automatically bundle Google Fonts</p> <p>The built-in privacy plugin makes it easy to use Google Fonts while complying with the General Data Protection Regulation (GDPR), by automatically downloading and self-hosting the web font files.</p>"},{"location":"Data%20Science/sample/#customization","title":"Customization","text":""},{"location":"Data%20Science/sample/#additional-fonts","title":"Additional fonts","text":"<p>If you want to load an (additional) font from another destination or override the system font, you can use an additional style sheet to add the corresponding <code>@font-face</code> definition:</p> <p>=== \":octicons-file-code-16: <code>docs/stylesheets/extra.css</code>\"</p> <pre><code>``` css\n@font-face {\n  font-family: \"&lt;font&gt;\";\n  src: \"...\";\n}\n```\n</code></pre> <p>=== \":octicons-file-code-16: <code>mkdocs.yml</code>\"</p> <pre><code>``` yaml\nextra_css:\n  - stylesheets/extra.css\n```\n</code></pre> <p>The font can then be applied to specific elements, e.g. only headlines, or globally to be used as the site-wide regular or monospaced font:</p> <p>=== \"Regular font\"</p> <pre><code>``` css\n:root {\n  --md-text-font: \"&lt;font&gt;\"; /* (1)! */\n}\n```\n\n1.  Always define fonts through CSS variables and not `font-family`, as\n    this would disable the system font fallback.\n</code></pre> <p>=== \"Monospaced font\"</p> <pre><code>``` css\n:root {\n  --md-code-font: \"&lt;font&gt;\";\n}\n```\n</code></pre>"}]}